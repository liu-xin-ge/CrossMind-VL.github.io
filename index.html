<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CrossMind-VL: Multi-Subject Mind-to-Video Decoding with Multimodal LLM Semantic Grounding</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/responsive_style.css">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>
        

<section class="hero">
  <div class="hero-body">
    <div class="container is-widescreen">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">CrossMind-VL</h1>
          <h2 class="subtitle is-3 publication-subtitle">Multi-Subject Mind-to-Video Decoding with Multimodal LLM Semantic Grounding</h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://orcid.org/0009-0006-5504-1768"><strong>Xuanliu Zhu</strong></a>,</span>&nbsp;
            <span class="author-block">
              <a href="https://orcid.org/0009-0003-8433-3864">Yiqiao Chai</a>&nbsp;
            </span>
            <span class="author-block">
              <a href="https://orcid.org/0009-0007-2220-7626">Runnan Li</a>&nbsp;
            </span> 
            <span class="author-block">
              <a href="https://orcid.org/0000-0003-0986-1336">Mingying Lan</a>&nbsp;
            </span> 
            <span class="author-block">
              <a href="https://orcid.org/0009-0002-4402-6643">Li Gao</a>&nbsp;
            </span>  



            
          </div>
          <br>
          <div class="is-size-4 publication-authors">
            <span class="author-block">Beijing University of Posts and Telecommunications</span>
          </div>
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://openreview.net/pdf?id=wAXsx2MYgV"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <img src="images/ICLR-logo.svg" style="height: 1.2em";>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              </span> -->
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/Isik-lab/SIfMRI_modeling"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Decoding dynamic visual information from brain activity remains
            challenging due to inter-subject neural heterogeneity, limited persubject
            data availability, and the substantial temporal resolution
            gap between fMRI signals (0.5Hz) and video dynamics (30Hz). Current
            approaches face persistent challenges in addressing these
            temporal mismatches, demonstrate limited capacity to integrate
            subject-specific neural patterns with shared representational frameworks,
            and lack adequate semantic granularity for aligning neural
            responses with visual content. To bridge these gaps, we propose
            CrossMind-VL, a framework addressing these limitations through
            three innovations: (1) a Dynamic Temporal Alignment module that
            resolves temporal mismatches via exponentially decayed multiframe
            fusion with adaptive decay coefficients; (2) a Brain Mixtureof-
            Experts architecture that combines subject-specific extractors
            with shared expert layers through parameter-efficient tri-modal
            contrastive learning; and (3) a Multi-perspective Semantic Hyper-
            Anchoring module that resolves cross-subject attention bias via
            multi-dimensional semantic decomposition, leveraging multimodal
            LLMs for fine-grained video semantic extraction—enabling the
            model to match individual attention patterns as different subjects
            naturally focus on distinct aspects of the same visual stimulus. This
            module boosts Top-10/Top-100 retrieval by 17.7%/6.6%. Experiments
            on two video-fMRI datasets demonstrate state-of-the-art performance,
            with 39%/30% improvements in Top-10/Top-100 accuracy
            over single-subject baselines and 27% gains against multi-subject
            models. The framework exhibits remarkable few-shot adaptability,retaining 97% performance when using only 10% training data for
            new subjects. Visualization analysis confirms this generalization
            capability stems from effective disentanglement of subject-specific
            and shared neural representations.
            
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <style>
    .video-row {
        display: flex;
        gap: 32px;
    justify-content: center;
    }
    .video-row video {
        width: 32%;
        height: auto;
    }
    </style>
    
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <br>
        <div class="video-row">
            <video src="dynamic/intro_1.mp4" autoplay muted loop></video>
            <video src="dynamic/intro_2.mp4" autoplay muted loop></video>
            <video src="dynamic/intro_3.mp4" autoplay muted loop></video>
        </div>
        <br>
        <div class="video-row">
            <video src="dynamic/methods_1.mp4" autoplay muted loop></video>
            <video src="dynamic/methods_2.mp4" autoplay muted loop></video>
            <video src="dynamic/methods_3.mp4" autoplay muted loop></video>
        </div>
        <br>
        <div class="video-row">
            <video src="dynamic/methods_4.mp4" autoplay muted loop></video>
            <video src="dynamic/methods_5.mp4" autoplay muted loop></video>
            <video src="dynamic/methods_6.mp4" autoplay muted loop></video>
        </div>
        <br><br><br>
      </div>
    </div> -->

    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Behavioral Feature Dimensions</h2>
        <br>
        <div class="content has-text-justified">
          <div style="text-align: justified;">
            <img src="static/figures/sim_judg/methods_behavior.png" alt="Behavioral Feature Dimensions">
            <br>
            <p class="caption" style="width: 100%; text-align: justify;">
                <b>Left:</b> Features are grouped into five domains: scene and object features (yellow), social primitive features (purple), social interaction features (blue), affective features (pink), and social appraisal features (orange).
                <b>Right:</b> Example frames from two videos are shown alongside their behavioral feature ratings, as indicated by the bar graphs. The colored bars correspond to the feature domains at left. Arrows highlight the dominant feature for each video, illustrating how different types of social events are distinguished by their behavioral profiles.
            </p>
            <br>
          </div>
        </div>
      </div>
    </div> -->

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">The framework of the proposed CrossMind-VL</h2>
        <br>
        <div class="content has-text-justified">
          <div style="text-align: justified;">
            <img src="static/figures/model.png" alt="Triplet Odd-One-Out Task">
            <br>
            <p class="caption" style="width: 100%; text-align: justify;">
                (a) Training Pipeline of CrossMind-VL. Initially, Qwen2-VL is employed
to annotate the stimulus video content in detail. Subsequently, CLIP is utilized to extract image and text features from the
videos. Finally, contrastive learning is applied to align the features of the three modalities—image, text, and brain signals—into
a unified representational space. (b) Multi-frame integration module.We aggregate fMRI signals within a fixed time window
using a weighted exponential decay strategy. (c) Architecture of the BrainMoE module. This module employs a decoupled
approach, utilizing subject-specific layers to extract unique information for each subject and subject-shared layers to extract
visual features common to all subjects. (d) Inference Pipeline of CrossMind-VL. By performing nearest neighbor search in CLIP
space across a video candidate pool, we can retrieve the original video using only brain activity patterns.
            </p>
            <br>
          </div>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Top-10 retrieval results on the CC2017 dataset</h2>
        <br>
        <div class="content has-text-justified">
          <div style="text-align: justified;">
            <img src="static/figures/retrieval.png" alt="RSA Overview">
            <br><br>
            <p class="caption" style="width: 100%; text-align: justify;">
               Retrieved videos are highlighted with red borders. It should
be noted that the natural images included in the schematic are sourced from the open-access dataset CC2017 (
https://purr.purdue.edu/publications/2809/1), which is licensed under the CC0 1.0 Universal Public Domain Dedication.
          </div>
        </div>
        <br><br>
      </div>
    </div>  

    <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
        <h2 class="title is-3">Behavioral and DNN Model Alignment with Human Similarity Judgments</h2>
        <br>
        <div class="content has-text-justified">
        <div style="text-align: justified;">
            <img src="static/figures/sim_judg/results_rsa.png" alt="Explained variance by behavioral features and DNN models">
            <br><br>
            <p class="caption" style="width: 100%; text-align: justify;">
            <b>Left:</b> Explained variance (R²) in human similarity judgments for individual behavioral features. Asterisks indicate features that reached statistical significance (<i>p</i> &lt; 0.05). The dashed horizontal line denotes the split-half reliability (0.252), representing the noise ceiling. The feature "intimacy" (a higher-order social-affective construct) explains the most variance, followed by "dominance," while lower-level perceptual and social interaction features explain less variance.
            <b>Right:</b> Explained variance for over 350 deep neural network models, grouped by modality (language, image, and video). Each dot represents the best-performing layer of a model. Language models exhibited the strongest overall alignment with human similarity judgments, with Multilingual MPNet-v2 as the top-performing language model. Among image models, CLIP (ViT-B/32) performed best, while for video models, X3D-M achieved the highest alignment with human judgments.
            </p>
        </div>
        </div>
        <br><br>
    </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Retrieval accuracy (%) on CC2017 dataset</h2>
        <div class="content has-text-justified">
          <div style="text-align: justified;">
            <br>
            <img src="static/figures/table1.png" alt="Variance Partitioning Methods" style="display: block; margin-left: auto; margin-right: auto;">
            <br>
            <p class="caption"><b>For the “small test set”, the chance-level accuracies for top-10 and top-100
accuracy are 0.83% and 8.3%, respectively. For the “large test set”, the chance-level accuracies for top-10 and top-100 accuracy
are 0.24% and 2.4%, respectively.
            </p>
            <br><br>
          </div>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
            <h2 class="title is-3">Retrieval accuracy (%) on HCP dataset</h2>
            <br>
            <div class="content has-text-justified">
                <div style="text-align: justified;">
                    <img src="static/figures/table2.png" alt="Explained Variance by Neural ROIs">
                    <br><br>
                    <p class="caption" style="width: 100%; text-align: justify;">
                    For the “small test set”, the chance-level accuracies for top-10 and top-100
accuracy are 3.29% and 32.9%, respectively. For the “large test set”, the chance-level accuracies for top-10 and top-100 accuracy
are 0.66% and 6.6%, respectively.
                    </p>
                </div>
            </div>
            <br><br>
        </div>
    </div>

    <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
        <h2 class="title is-3">Conclusion</h2>
        <div class="content has-text-justified">
        <div style="text-align: justified;">
            <p class="caption">
            We present CrossMind-VL, a unified framework for cross-subject
brain decoding of dynamic visual stimuli that addresses three key
challenges:
            <b>(1)</b> the temporal mismatch between fMRI hemodynamics
(0.5 Hz) and video dynamics (30 Hz)
            <b>(2)</b> the joint modeling
of subject-specific and shared neural representations
            <b>(3)</b> the
limitations of conventional semantic alignment for dynamic scenes.
Our solution introduces: 
<b>(1)</b> an exponentially decayed multi-frame
fusion mechanism for temporal alignment. <b>(2)</b> a hierarchical architecture
combining parameterized subject-specific layers with a
gated MoE shared representation layer.<b>(3)</b> a multi-granularity
alignment paradigm using Qwen-VL to establish an image-textbrain
contrastive learning space.
            </p>
        </div>
        </div>
    </div>
    </div>

</section>



</body>
</html>
